model:
  model_type: "bert"
  vocab_size: 28996 # bert-base-cased vocab size
  hidden_size: 128
  num_attention_heads: 2
  num_hidden_layers: 2
  intermediate_size: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  layer_norm_eps: 1e-12
  initializer_range: 0.02

training:
  # dataset_name: "wikitext"
  tokenizer_name: "bert-base-cased"
  train_jsonl_path: "/Users/yashgajjar/work-space/yash/DataGen/output/mlm_google_200_20250515_172445.jsonl"
  val_jsonl_path: "/Users/yashgajjar/work-space/yash/DataGen/output/mlm_data.jsonl"
  max_seq_length: 128
  batch_size: 32
  num_epochs: 1
  learning_rate: 5e-5
  save_model_path: "./trained_models/bert_model.pt"
  device: "cuda"  # or "cpu" if no GPU is available