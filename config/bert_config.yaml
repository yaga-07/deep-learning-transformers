model:
  model_type: "bert"
  vocab_size: 30522
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 12
  intermediate_size: 3072
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  layer_norm_eps: 1e-12
  initializer_range: 0.02

training:
  dataset_name: "wikitext"
  max_seq_length: 128
  batch_size: 32
  num_epochs: 1
  learning_rate: 5e-5
  save_model_path: "./trained_models/bert_model.pt"
  device: "cuda"  # or "cpu" if no GPU is available